{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BLANK\n",
    "\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import ast\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import skimage.io as io\n",
    "from shapely import Polygon\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import cuda\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm, trange\n",
    "from importlib import reload\n",
    "\n",
    "# Clone georip repo here: https://www.github.com/joeletho/georip.git\n",
    "\n",
    "# Cloned repo directory\n",
    "sys.path.append(\"path/to/georip\")\n",
    "\n",
    "import georip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "\n",
    "print(reload(georip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "\n",
    "has_gpu = cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if has_gpu else 'cpu')\n",
    "print(device)\n",
    "if has_gpu:\n",
    "    print(cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example directory structure:\n",
    "```\n",
    "Root\n",
    "  ├── GEORIP_YOLO\n",
    "  |         ├── datasets\n",
    "  |         ├── models\n",
    "  ├── NDVI\n",
    "  ├── QGIS\n",
    "  ├── Readme.txt\n",
    "  ├── Shapefiles\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "path_map = {}\n",
    "\n",
    "# Change this to your project path\n",
    "path_map['ROOT'] = Path(\"path/to/project/root\")\n",
    "\n",
    "path_map['PROJECT_NAME'] = 'GEORIP_YOLO'\n",
    "path_map['georip'] = path_map['ROOT'] / path_map[\"PROJECT_NAME\"]\n",
    "path_map['GEORIP_DS'] = path_map['georip'] / 'datasets'\n",
    "path_map['GEORIP_MODELS'] = path_map['georip'] / 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "def make_directories(paths_map, verbose=True, exist_ok=False):\n",
    "    if verbose:\n",
    "        print(\"Creating directory structure\")\n",
    "    for name, path in paths_map.items():\n",
    "        if isinstance(path, Path):\n",
    "            if path.is_file() or len(path.suffix) > 0:\n",
    "                paths_map[name] = path.resolve()\n",
    "            else:\n",
    "                path = path.resolve()\n",
    "                paths_map[name] = path\n",
    "                path.mkdir(parents=True, exist_ok=exist_ok)\n",
    "                if verbose:\n",
    "                    print('  ',path)\n",
    "    if verbose:\n",
    "        print(\"Complete\")\n",
    "\n",
    "def make_project_paths(root,*, verbose=True, exist_ok=False):    \n",
    "    paths = {'NDVI': Path(root, 'NDVI', 'NDVI Difference Rasters')}\n",
    "    paths['SHAPE_FILES'] = Path(root, 'Shapefiles')\n",
    "\n",
    "    paths['GEORIP_DS_META'] = path_map['GEORIP_DS'] / 'meta'\n",
    "    paths['GEORIP_DS_CSV'] = paths['GEORIP_DS_META'] / 'csv'\n",
    "    paths['GEORIP_DS_SHP'] = paths['GEORIP_DS_META'] / 'shp'\n",
    "    \n",
    "    # Data\n",
    "    paths['PRED_SHP'] = paths['SHAPE_FILES'] / 'ModelPredictions'\n",
    "    paths['SHPZ10_SHP'] = paths['SHAPE_FILES'] / 'Treatments_UTMz10_Only_08-18-24' / 'Treatments_UTMz10_Only_08-18-24_GEE.shp'\n",
    "    paths['SHPZ10_SHP_BACKGROUND'] = paths['SHAPE_FILES'] / 'facts_calmapper_utmz10n/facts_calmapper_utmz10n.shp'\n",
    "    for name, path in paths.items():\n",
    "        path_map[name] = path\n",
    "    make_directories(paths, verbose=verbose, exist_ok=exist_ok)\n",
    "\n",
    "        \n",
    "def make_dataset_paths(ds_root, models_root, model_name, *,verbose=True, exist_ok=False):\n",
    "    ds_root = Path(ds_root)\n",
    "    models_root = Path(models_root)\n",
    "    paths = {}\n",
    "        \n",
    "    paths['MODEL_NAME'] = model_name\n",
    "    paths['GEORIP_MODEL'] = models_root / paths['MODEL_NAME']\n",
    "    paths['GEORIP_DS_MODEL'] = ds_root / paths['MODEL_NAME']\n",
    "    paths['GEORIP_DS_MODEL_META'] = paths['GEORIP_DS_MODEL'] / 'meta'\n",
    "    paths['GEORIP_DS_MODEL_SHP'] = paths['GEORIP_DS_MODEL_META'] / 'shp'\n",
    "    paths['GEORIP_DS_MODEL_CSV'] = paths['GEORIP_DS_MODEL_META'] / 'csv'\n",
    "    \n",
    "    paths['GEORIP_DS_DATA'] = paths['GEORIP_DS_MODEL'] / 'meta'\n",
    "    paths['GEORIP_DS_CONFIG_FILE'] = paths['GEORIP_DS_MODEL'] / 'config' / 'data.yaml'\n",
    "    paths['GEORIP_DS_YOLO_DATA_FILE'] = paths['GEORIP_DS_DATA'] / 'yolo_ndvi_ds.csv'\n",
    "    \n",
    "    # Images and labels\n",
    "    paths['GEORIP_DS_IMAGES'] = paths['GEORIP_DS_MODEL'] / 'images'\n",
    "    paths['GEORIP_DS_LABELS'] = paths['GEORIP_DS_MODEL'] / 'labels'\n",
    "    paths['GEORIP_DS_LABELS_GENERATED'] = paths['GEORIP_DS_LABELS'] / 'generated'\n",
    "    \n",
    "    paths['GEORIP_DS_tileS'] = paths[\"GEORIP_DS_IMAGES\"] / 'tiles'\n",
    "    paths['GEORIP_DS_PNGS'] = paths[\"GEORIP_DS_IMAGES\"] / 'png'\n",
    "    paths['GEORIP_DS_TIFS'] = paths[\"GEORIP_DS_IMAGES\"] / 'tif'\n",
    "    \n",
    "    paths['GEORIP_DS_IMAGES_TRAIN'] = paths['GEORIP_DS_IMAGES'] / 'train'\n",
    "    paths['GEORIP_DS_IMAGES_TEST'] = paths['GEORIP_DS_IMAGES'] / 'test'\n",
    "    paths['GEORIP_DS_IMAGES_VAL'] = paths['GEORIP_DS_IMAGES'] / 'val'\n",
    "    \n",
    "    paths['GEORIP_DS_LABELS_TRAIN'] = paths['GEORIP_DS_LABELS'] / 'train'\n",
    "    paths['GEORIP_DS_LABELS_TEST'] = paths['GEORIP_DS_LABELS'] / 'test'\n",
    "    paths['GEORIP_DS_LABELS_VAL'] = paths['GEORIP_DS_LABELS'] / 'val'\n",
    "\n",
    "    # Metadata\n",
    "\n",
    "    # Zone 10\n",
    "    paths['CSVZ10'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_UTMz10.csv'\n",
    "    paths['CSVZ10_NORM'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_UTMz10_normalized.csv'\n",
    "    paths['CSVZ10_CLEANED'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_UTMz10_normalized_cleaned.csv'\n",
    "    paths['CSVZ10_TILED'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_UTMz10_normalized_tiled.csv'\n",
    "    paths['CSVZ10_TILED_LABELS_UTM'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_z10utm_tiled_labels.csv'\n",
    "    paths['CSVZ10_TILED_LABELS_PIXEL'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_z10pixel_tiled_labels.csv'\n",
    "    paths['CSVZ10_TILED_LABELS_PIXEL_ENCODED'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_z10pixel_tiled_labels_encoded.csv'\n",
    "    paths['CSVZ10_TILED_LABELS_PREYOLO'] = paths['GEORIP_DS_MODEL_CSV'] / 'Treatments_z10pixel_tiles_labels_encoded_preyolo.csv'\n",
    "    \n",
    "    for name, path in paths.items():\n",
    "        path_map[name] = path\n",
    "    \n",
    "    make_directories(paths, verbose=verbose, exist_ok=exist_ok)\n",
    "\n",
    "    path_map['SHPZ10_PRED_SHP'] = path_map['PRED_SHP'] / f\"Treatmentsz10_{paths['MODEL_NAME']}.shp\"\n",
    "    path_map['SHPZ11_PRED_SHP'] = path_map['PRED_SHP'] / f\"Treatmentsz11_{paths['MODEL_NAME']}.shp\"\n",
    "\n",
    "make_project_paths(path_map['ROOT'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "# Class encoder function\n",
    "def classify(row):\n",
    "    geom = row.get('geometry')\n",
    "    return (0, \"Treatment\") if geom is not None and not geom.is_empty and geom.area > 1 else (-1, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "import pandas as pd\n",
    "\n",
    "def parse_dates(df, column_name):\n",
    "    try:\n",
    "        return pd.to_datetime(df[column_name], format='mixed', errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Mixed format failed for {column_name}: {e}\")\n",
    "\n",
    "# Fix field name mismatch\n",
    "background_df = georip.io.load_shapefile(path_map['SHPZ10_SHP_BACKGROUND'])\n",
    "if background_df.get(\"StartDate\") is not None:\n",
    "    background_df[\"StartYear\"] = parse_dates(background_df, 'StartDate').dt.year.astype(pd.Int64Dtype())\n",
    "    background_df[\"EndYear\"] = parse_dates(background_df, 'EndDate').dt.year.astype(pd.Int64Dtype())\n",
    "    background_df = background_df.drop(columns=[\"StartDate\", \"EndDate\"])\n",
    "    georip.io.save_as_shp(background_df, path_map['SHPZ10_SHP_BACKGROUND'], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BLANK\n",
    "from georip.modeling.utils import DatasetSplitMode\n",
    "\n",
    "# Dataset configuration settings\n",
    "CHIP_SIZE = [320, 640]\n",
    "YEARS=[2019, 2020, 2021, 2022, 2023]\n",
    "SPLIT=0.70\n",
    "SPLIT_MODE=[DatasetSplitMode.All, DatasetSplitMode.Collection]\n",
    "BACKGROUND_RATIO=1.0\n",
    "SHUFFLE_SPLIT=[False, True]\n",
    "TREATMENTS = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "single_treatment = False\n",
    "\n",
    "# Error list\n",
    "errors = []\n",
    "\n",
    "# Configure TQDM progress bar\n",
    "total_updates = (\n",
    "    len(TREATMENTS) *\n",
    "    (len(YEARS)-1) *\n",
    "    len(SPLIT_MODE) *\n",
    "    len(SHUFFLE_SPLIT) *\n",
    "    len(CHIP_SIZE)\n",
    "    )\n",
    "root_pbar = trange(total_updates)\n",
    "updates = 0\n",
    "\n",
    "# Get tile size\n",
    "for size in IMG_SIZE:\n",
    "    # Get treatment\n",
    "    for tmt_idx, treatment in enumerate(TREATMENTS): \n",
    "        if not single_treatment and tmt_idx > 0:\n",
    "            break\n",
    "        # Get years\n",
    "        for year_idx in range(len(YEARS)-1):\n",
    "            years = (YEARS[year_idx], YEARS[year_idx+1])\n",
    "            years = None if years is None or years[0] is None else years\n",
    "            \n",
    "            # Get split mode\n",
    "            for mode in SPLIT_MODE:\n",
    "                # Get split flag\n",
    "                for shuffle_split in SHUFFLE_SPLIT:\n",
    "                    # Get the background bias\n",
    "                    for background_ratio in BACKGROUND_RATIO:\n",
    "                        ndvi_has_suffix = ndvi_path.name in ['vv', 'vh']\n",
    "                        ndvi_parts = ndvi_path.parts\n",
    "                        ndvi_name = ndvi_parts[-2].split()[0] if ndvi_has_suffix else ndvi_parts[-1].split()[0]\n",
    "\n",
    "                        if include_sentinel and ndvi_idx > 0 and ndvi_has_suffix:\n",
    "                            ndvi_name = '_'.join([ndvi_name, ndvi_parts[-1]])\n",
    "                        \n",
    "                        years_str = f\"{str(years) if years is None else f'{str(years[0])}to{str(years[1])}'}\"\n",
    "                        \n",
    "                        # Update pbar message\n",
    "                        model_info = f\"DS: {ndvi_name}, T: {'all' if treatment == 0 else str(treatment) if len(TREATMENTS) == 1 else '_'.join([str(tmt) for tmt in TREATMENTS])}, Y: {years_str}, SM: {mode}, S: {SPLIT}, SS: {shuffle_split}, B: {str(background_ratio)}\"\n",
    "                        root_pbar.set_description(f\"Creating dataset {updates+1}: {model_info}\")\n",
    "        \n",
    "                        # Create the model name and create its repository\n",
    "                        path_map['MODEL_NAME'] = f\"yolo_treatments={'all' if treatment == 0 else str(treatment)}_years={years_str}_imgsz={size if size is not None else 'Default'}_split={int(SPLIT*100)}_mode={mode}_shuffle-split={shuffle_split}_bg={str(background).replace('.','_')}{'' if not SHUFFLE_BACKGROUND else '_shuffle-bg=True'}\"\n",
    "                        make_dataset_paths(\n",
    "                            path_map['GEORIP_DS'], \n",
    "                            path_map['GEORIP_MODELS'],  \n",
    "                            path_map['MODEL_NAME'], \n",
    "                            verbose=False, \n",
    "                            exist_ok=True\n",
    "                        )\n",
    "        \n",
    "                        # Load the master shapefile\n",
    "                        shpz10 = georip.io.load_shapefile(path_map['SHPZ10_SHP'])\n",
    "                        if treatment == 0:\n",
    "                            # All treatments\n",
    "                            shpz10 = shpz10[shpz10['TreatmentT'] != 8]\n",
    "                        else:\n",
    "                            # Individual treatment\n",
    "                            shpz10 = shpz10[shpz10['TreatmentT'] == treatment]\n",
    "        \n",
    "                        # Rename these rows to align filenames that are parsed later\n",
    "                        shpz10.loc[shpz10['Subregion'] == \"Humboldt\", \"Subregion\"] = 'Humboldt4'\n",
    "                        if years is not None:\n",
    "                            shpz10 = shpz10[shpz10['StartYear'] == years[0]]\n",
    "                            shpz10 = shpz10[shpz10['EndYear'] == years[1]]\n",
    "        \n",
    "                        # Declare the path of the base files used for this dataset\n",
    "                        BASE_FILEPATH = Path(f'base_years={years_str}', 'Treatments_UTMz10_Only_08-18-24')\n",
    "                        \n",
    "                        # Save the files\n",
    "                        georip.io.save_as_csv(shpz10, path_map['GEORIP_DS_CSV'] / BASE_FILEPATH.with_suffix('.csv'), exist_ok=True)\n",
    "                        georip.io.save_as_shp(shpz10, path_map['GEORIP_DS_SHP'] / BASE_FILEPATH.with_suffix('.shp'), exist_ok=True)\n",
    "\n",
    "                        try:\n",
    "                            # Make the dataset using the base shapefile\n",
    "                            yolo_ds = georip.datasets.YOLONDVIDifferenceDataset.create(\n",
    "                                source = shp_source,\n",
    "                                source_images_dir = ndvi_path,\n",
    "                                output_dir = path_map['GEORIP_DS_MODEL'],\n",
    "                                region_column = ['Subregion', 'gee_region'],\n",
    "                                year_start_column = \"StartYear\",\n",
    "                                year_end_column = \"EndYear\",\n",
    "                                geometry_column = \"geometry\",\n",
    "                                years = years,\n",
    "                                background = False,\n",
    "                                background_ratio = background_ratio,\n",
    "                                split=mode,\n",
    "                                split_ratio= SPLIT,  # 0.7 (70/30)\n",
    "                                shuffle_split = shuffle_split,  # True/False\n",
    "                                generate_labels = True,\n",
    "                                generate_train_data = True,  # True/False\n",
    "                                tile_size=size,\n",
    "                                translate_xy = True,  # True/False\n",
    "                                class_encoder= encode_classes,  # None or callback(row)\n",
    "                                exist_ok = True,  # True/False\n",
    "                                clear_output_dir = True,  # True/False\n",
    "                                save_shp = True,  # True/False\n",
    "                                save_gpkg = True,  # True/False\n",
    "                                save_csv = True,  # True/False\n",
    "                                pbar_leave = True,  # True/False\n",
    "                                convert_to_png = True,\n",
    "                                use_segments = True,\n",
    "                                num_workers = 8,\n",
    "                                preserve_fields = [\"TreatmentT\"]\n",
    "                            )            \n",
    "                    if len(yolo_ds.images) < 40:\n",
    "                        # If the size is too small the dataset encounters issues so we limit it to a \n",
    "                        # size that may provide a decent number of images for training\n",
    "                        raise ValueError(\"Too few images to be viable dataset\")\n",
    "\n",
    "                    # (Optional) Change the root path of the dataset to the target directory where it will be used later on\n",
    "                    yolo_ds.generate_yaml_file(\n",
    "                        root_abs_path=Path(SOME_OTHER_PATH, path_map['MODEL_NAME']),\n",
    "                        dest_abs_path=path_map['GEORIP_DS_MODEL'] / 'config',\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    # Append the error message and remove the created files\n",
    "                    errors.append(f\"{path_map['MODEL_NAME']}: {e}\")\n",
    "                    shutil.rmtree(path_map['GEORIP_DS_MODEL'])\n",
    "\n",
    "                # Updae the pbar and update counter\n",
    "                root_pbar.update()\n",
    "                updates += 1\n",
    "\n",
    "root_pbar.set_description(f\"Dataset completed with {len(errors)} errors.\")\n",
    "root_pbar.refresh()\n",
    "root_pbar.close()\n",
    "\n",
    "if len(errors) > 0:\n",
    "    print(\"The following errors occurred:\\n\", \"\\n\".join(errors), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK\n",
    "yolo_ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
